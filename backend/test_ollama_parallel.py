#!/usr/bin/env python3
"""
Script de test pour vÃ©rifier la parallÃ©lisation Ollama.
Lance plusieurs requÃªtes simultanÃ©es et mesure le temps.
"""

import os
import sys
import time
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Configuration
OLLAMA_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
MODEL = os.getenv("OLLAMA_MODEL", "ministral-3:3b")
NUM_REQUESTS = int(os.getenv("NUM_REQUESTS", 12))  # Nombre de requÃªtes parallÃ¨les Ã  tester
PROMPT = "DÃ©cris briÃ¨vement la Joconde en 2 phrases."

print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         TEST DE PARALLÃ‰LISATION OLLAMA                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ URL: {OLLAMA_URL:<52} â•‘
â•‘ ModÃ¨le: {MODEL:<49} â•‘
â•‘ RequÃªtes parallÃ¨les: {NUM_REQUESTS:<37} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# Compteur de requÃªtes actives
active_count = 0
active_lock = threading.Lock()
max_concurrent = 0

def make_request(request_id: int):
    """Fait une requÃªte Ã  Ollama et retourne le temps d'exÃ©cution."""
    global active_count, max_concurrent
    
    start = time.time()
    
    with active_lock:
        active_count += 1
        if active_count > max_concurrent:
            max_concurrent = active_count
        current_active = active_count
    
    print(f"[REQ {request_id:02d}] ğŸš€ DÃ©marrage (actives: {current_active})")
    
    try:
        response = requests.post(
            f"{OLLAMA_URL}/api/chat",
            json={
                "model": MODEL,
                "messages": [{"role": "user", "content": f"{PROMPT} (req #{request_id})"}],
                "stream": False,
                "options": {
                    "num_predict": 100,  # Limiter la rÃ©ponse
                    "num_gpu": 0,  # CPU only
                }
            },
            timeout=120
        )
        
        with active_lock:
            active_count -= 1
        
        duration = time.time() - start
        
        if response.status_code == 200:
            print(f"[REQ {request_id:02d}] âœ… OK en {duration:.2f}s")
            return {'id': request_id, 'duration': duration, 'success': True}
        else:
            print(f"[REQ {request_id:02d}] âŒ Erreur HTTP {response.status_code}")
            return {'id': request_id, 'duration': duration, 'success': False, 'error': response.status_code}
            
    except Exception as e:
        with active_lock:
            active_count -= 1
        duration = time.time() - start
        print(f"[REQ {request_id:02d}] âŒ Exception: {e}")
        return {'id': request_id, 'duration': duration, 'success': False, 'error': str(e)}


def main():
    global max_concurrent
    
    # VÃ©rifier Ollama
    print("ğŸ” VÃ©rification Ollama...")
    try:
        r = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        if r.status_code != 200:
            print(f"âŒ Ollama non accessible: HTTP {r.status_code}")
            sys.exit(1)
        models = r.json().get('models', [])
        print(f"âœ… Ollama OK - {len(models)} modÃ¨les disponibles")
    except Exception as e:
        print(f"âŒ Ollama non accessible: {e}")
        sys.exit(1)
    
    # Test sÃ©quentiel d'abord (baseline)
    print("\n" + "="*60)
    print("ğŸ“ TEST 1: RequÃªte SÃ‰QUENTIELLE (baseline)")
    print("="*60)
    
    seq_start = time.time()
    result = make_request(0)
    seq_duration = result['duration']
    print(f"â±ï¸ Temps sÃ©quentiel: {seq_duration:.2f}s")
    
    # Test parallÃ¨le
    print("\n" + "="*60)
    print(f"ğŸš€ TEST 2: {NUM_REQUESTS} requÃªtes PARALLÃˆLES")
    print("="*60)
    
    max_concurrent = 0
    parallel_start = time.time()
    
    with ThreadPoolExecutor(max_workers=NUM_REQUESTS) as executor:
        futures = [executor.submit(make_request, i+1) for i in range(NUM_REQUESTS)]
        results = [f.result() for f in as_completed(futures)]
    
    parallel_duration = time.time() - parallel_start
    
    # Analyse
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    avg_duration = sum(r['duration'] for r in successful) / len(successful) if successful else 0
    
    print("\n" + "="*60)
    print("ğŸ“Š RÃ‰SULTATS")
    print("="*60)
    print(f"âœ… RÃ©ussies: {len(successful)}/{NUM_REQUESTS}")
    print(f"âŒ Ã‰chouÃ©es: {len(failed)}/{NUM_REQUESTS}")
    print(f"â±ï¸ Temps total parallÃ¨le: {parallel_duration:.2f}s")
    print(f"â±ï¸ Temps moyen par requÃªte: {avg_duration:.2f}s")
    print(f"ğŸ”¥ Max requÃªtes simultanÃ©es: {max_concurrent}")
    
    # Calcul du speedup
    expected_sequential = seq_duration * NUM_REQUESTS
    speedup = expected_sequential / parallel_duration if parallel_duration > 0 else 0
    
    print(f"\nğŸ“ˆ PERFORMANCE:")
    print(f"   Temps sÃ©quentiel estimÃ©: {expected_sequential:.2f}s")
    print(f"   Temps parallÃ¨le rÃ©el: {parallel_duration:.2f}s")
    print(f"   Speedup: {speedup:.2f}x")
    
    if speedup < 1.5:
        print("\nâš ï¸ ATTENTION: Speedup faible! Ollama ne semble pas traiter les requÃªtes en parallÃ¨le.")
        print("   VÃ©rifiez que OLLAMA_NUM_PARALLEL est bien configurÃ© et que le container est redÃ©marrÃ©.")
    elif speedup < NUM_REQUESTS * 0.5:
        print(f"\nâš ï¸ Speedup modÃ©rÃ© ({speedup:.1f}x sur {NUM_REQUESTS} workers possibles)")
        print("   La parallÃ©lisation fonctionne partiellement.")
    else:
        print(f"\nâœ… Excellent! ParallÃ©lisation efficace ({speedup:.1f}x)")
    
    print("\nğŸ¯ Recommandation:")
    print(f"   Pour utiliser tous les {os.cpu_count() or 'N/A'} CPU, configurez OLLAMA_NUM_PARALLEL={os.cpu_count()//2 if os.cpu_count() else 12}")


if __name__ == "__main__":
    main()

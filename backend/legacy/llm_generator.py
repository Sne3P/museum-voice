"""
GÃ©nÃ©rateur de narrations avec LLM (Ollama, Groq, OpenAI)
GÃ©nÃ¨re des narrations UNIQUES basÃ©es sur le contenu RAG
"""

import os
from typing import Dict, List, Any, Optional
import requests
import json

from .rag_engine_postgres import get_rag_engine
from .db_postgres import get_artwork


class LLMNarrationGenerator:
    """
    GÃ©nÃ©rateur de narrations uniques via LLM
    Utilise le RAG pour rÃ©cupÃ©rer le contenu pertinent, puis gÃ©nÃ¨re avec LLM
    """
    
    def __init__(self, provider: str = "groq"):
        """
        Args:
            provider: 'ollama', 'groq', ou 'openai'
        """
        self.provider = provider.lower()
        self.rag_engine = get_rag_engine()
        
        # Configuration selon le provider
        if self.provider == "ollama":
            self.api_url = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
            self.model = os.getenv("OLLAMA_MODEL", "mistral")
            
        elif self.provider == "groq":
            self.api_key = os.getenv("GROQ_API_KEY")
            self.api_url = "https://api.groq.com/openai/v1/chat/completions"
            self.model = os.getenv("GROQ_MODEL", "mixtral-8x7b-32768")
            
        elif self.provider == "openai":
            self.api_key = os.getenv("OPENAI_API_KEY")
            self.api_url = "https://api.openai.com/v1/chat/completions"
            self.model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        
        print(f"ðŸ¤– LLM Generator: {self.provider} ({self.model})")
    
    def generate_narration(self, 
                          oeuvre_id: int,
                          age_cible: str,
                          thematique: str,
                          style_texte: str,
                          max_length: int = 800) -> str:
        """
        GÃ©nÃ¨re une narration unique via LLM
        
        Args:
            oeuvre_id: ID de l'Å“uvre
            age_cible: 'enfant', 'ado', 'adulte', 'senior'
            thematique: 'technique_picturale', 'biographie', 'historique'
            style_texte: 'analyse', 'decouverte', 'anecdote'
            max_length: Longueur max en caractÃ¨res
        
        Returns:
            Narration gÃ©nÃ©rÃ©e
        """
        
        # 1. RÃ©cupÃ©rer l'Å“uvre
        artwork = get_artwork(oeuvre_id)
        if not artwork:
            raise ValueError(f"Å’uvre {oeuvre_id} non trouvÃ©e")
        
        # 2. Construire la requÃªte RAG selon la thÃ©matique
        query = self._build_rag_query(artwork, thematique)
        
        # 3. RÃ©cupÃ©rer le contenu via RAG
        rag_content = self._get_rag_content(oeuvre_id, query)
        
        # 4. Construire le prompt pour le LLM
        prompt = self._build_llm_prompt(
            artwork=artwork,
            rag_content=rag_content,
            age_cible=age_cible,
            thematique=thematique,
            style_texte=style_texte,
            max_length=max_length
        )
        
        # 5. GÃ©nÃ©rer avec le LLM
        narration = self._call_llm(prompt, max_length)
        
        return narration
    
    def _build_rag_query(self, artwork: Dict, thematique: str) -> str:
        """Construit la requÃªte RAG selon la thÃ©matique"""
        
        title = artwork.get('title', '')
        artist = artwork.get('artist', '')
        
        if thematique == 'technique_picturale':
            return f"{title} {artist} technique matÃ©riaux composition couleurs style peinture"
        
        elif thematique == 'biographie':
            return f"{artist} {title} vie carriÃ¨re formation influences contexte artistique"
        
        elif thematique == 'historique':
            return f"{title} {artist} contexte historique Ã©poque commande rÃ©ception critique postÃ©ritÃ©"
        
        return f"{title} {artist}"
    
    def _get_rag_content(self, oeuvre_id: int, query: str, top_k: int = 5) -> str:
        """RÃ©cupÃ¨re le contenu pertinent via RAG"""
        
        try:
            # Rechercher les chunks similaires
            results = self.rag_engine.search_similar_chunks(
                query=query,
                oeuvre_id=oeuvre_id,
                top_k=top_k,
                threshold=0.2
            )
            
            if not results:
                # Fallback: prendre tous les chunks de l'Å“uvre
                from .db_postgres import get_artwork_chunks
                chunks = get_artwork_chunks(oeuvre_id)
                return '\n\n'.join([c['chunk_text'] for c in chunks[:3]])
            
            # Assembler le contenu des meilleurs chunks
            content_parts = []
            for result in results:
                content_parts.append(result['chunk_text'])
            
            return '\n\n'.join(content_parts)
            
        except Exception as e:
            print(f"âš ï¸  Erreur RAG: {e}")
            # Fallback basique
            from .db_postgres import get_artwork_chunks
            chunks = get_artwork_chunks(oeuvre_id)
            return '\n\n'.join([c['chunk_text'] for c in chunks[:3]]) if chunks else ""
    
    def _build_llm_prompt(self, 
                         artwork: Dict,
                         rag_content: str,
                         age_cible: str,
                         thematique: str,
                         style_texte: str,
                         max_length: int) -> str:
        """Construit le prompt pour le LLM"""
        
        title = artwork.get('title', 'cette Å“uvre')
        artist = artwork.get('artist', 'l\'artiste')
        
        # Profils d'Ã¢ge
        age_instructions = {
            'enfant': "Utilise un vocabulaire simple et des phrases courtes. Sois ludique et encourageant. Maximum 400 mots.",
            'ado': "Utilise un langage accessible et dynamique. Sois captivant. Maximum 500 mots.",
            'adulte': "Utilise un langage riche et prÃ©cis. Sois informatif et analytique. Maximum 600 mots.",
            'senior': "Utilise un vocabulaire enrichi et des phrases dÃ©veloppÃ©es. Sois respectueux et approfondi. Maximum 700 mots."
        }
        
        # ThÃ©matiques
        theme_instructions = {
            'technique_picturale': "Concentre-toi sur les techniques, matÃ©riaux, composition, couleurs et style artistique.",
            'biographie': "Parle de l'artiste, sa vie, sa carriÃ¨re, ses influences et le contexte de crÃ©ation de l'Å“uvre.",
            'historique': "Explique le contexte historique, l'Ã©poque, la commande, la rÃ©ception critique et la postÃ©ritÃ©."
        }
        
        # Styles
        style_instructions = {
            'analyse': "Adopte un ton analytique et structurÃ©. Observe, dÃ©cris et interprÃ¨te.",
            'decouverte': "Adopte un ton engageant et exploratoire. Invite Ã  la dÃ©couverte et Ã  l'Ã©merveillement.",
            'anecdote': "Raconte des histoires et anecdotes fascinantes. Rends l'Å“uvre vivante et captivante."
        }
        
        prompt = f"""Tu es un guide culturel expert dans un musÃ©e. Tu dois crÃ©er une narration audio unique et captivante pour l'Å“uvre "{title}" de {artist}.

**Profil du visiteur:** {age_cible}
**ThÃ©matique:** {thematique}
**Style:** {style_texte}

**Instructions:**
- {age_instructions.get(age_cible, '')}
- {theme_instructions.get(thematique, '')}
- {style_instructions.get(style_texte, '')}
- GÃ©nÃ¨re une narration FLUIDE et NATURELLE, comme si tu parlais directement au visiteur
- NE COMMENCE PAS par "Analyse :", "Bienvenue", ou des formules gÃ©nÃ©riques
- Va DIRECTEMENT au contenu
- Utilise UNIQUEMENT les informations ci-dessous (contenu rÃ©el de l'Å“uvre)
- Sois UNIQUE et ORIGINAL - ne rÃ©pÃ¨te pas de formules toutes faites

**Informations sur l'Å“uvre (extraites du PDF):**
{rag_content}

**Narration (commence directement, maximum {max_length} caractÃ¨res):**"""

        return prompt
    
    def _call_llm(self, prompt: str, max_length: int = 800) -> str:
        """Appelle le LLM pour gÃ©nÃ©rer la narration"""
        
        if self.provider == "ollama":
            return self._call_ollama(prompt, max_length)
        elif self.provider == "groq":
            return self._call_groq(prompt, max_length)
        elif self.provider == "openai":
            return self._call_openai(prompt, max_length)
        else:
            raise ValueError(f"Provider non supportÃ©: {self.provider}")
    
    def _call_ollama(self, prompt: str, max_length: int) -> str:
        """Appel Ã  Ollama local"""
        
        try:
            response = requests.post(
                f"{self.api_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": max_length // 3  # ~3 chars par token
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '').strip()
            else:
                print(f"âŒ Erreur Ollama: {response.status_code}")
                return self._fallback_generation(prompt)
                
        except Exception as e:
            print(f"âŒ Erreur Ollama: {e}")
            return self._fallback_generation(prompt)
    
    def _call_groq(self, prompt: str, max_length: int) -> str:
        """Appel Ã  Groq API"""
        
        if not self.api_key:
            print("âš ï¸  GROQ_API_KEY non dÃ©finie")
            return self._fallback_generation(prompt)
        
        try:
            response = requests.post(
                self.api_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7,
                    "max_tokens": max_length // 3
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content'].strip()
            else:
                print(f"âŒ Erreur Groq: {response.status_code}")
                return self._fallback_generation(prompt)
                
        except Exception as e:
            print(f"âŒ Erreur Groq: {e}")
            return self._fallback_generation(prompt)
    
    def _call_openai(self, prompt: str, max_length: int) -> str:
        """Appel Ã  OpenAI API"""
        
        if not self.api_key:
            print("âš ï¸  OPENAI_API_KEY non dÃ©finie")
            return self._fallback_generation(prompt)
        
        try:
            response = requests.post(
                self.api_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7,
                    "max_tokens": max_length // 3
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content'].strip()
            else:
                print(f"âŒ Erreur OpenAI: {response.status_code}")
                return self._fallback_generation(prompt)
                
        except Exception as e:
            print(f"âŒ Erreur OpenAI: {e}")
            return self._fallback_generation(prompt)
    
    def _fallback_generation(self, prompt: str) -> str:
        """GÃ©nÃ©ration de secours si LLM Ã©choue"""
        
        # Extraire le contenu du prompt (aprÃ¨s "Informations sur l'Å“uvre")
        if "Informations sur l'Å“uvre" in prompt:
            parts = prompt.split("Informations sur l'Å“uvre")
            if len(parts) > 1:
                content = parts[1].split("**Narration")[0].strip()
                # Nettoyer
                content = content.replace(":**", "").replace("(extraites du PDF):", "").strip()
                return content[:800]
        
        return "Contenu non disponible. Veuillez rÃ©essayer."


# Instance globale
_generator = None

def get_llm_generator(provider: str = "groq") -> LLMNarrationGenerator:
    """Singleton pour LLM Generator"""
    global _generator
    if _generator is None:
        _generator = LLMNarrationGenerator(provider=provider)
    return _generator
